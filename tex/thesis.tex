\documentclass[12pt]{article}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{graphicx}
\graphicspath{ {images/} }

\linespread{1.5}
      	
\setlength{\oddsidemargin}{0.25in}
\setlength{\textwidth}{6in}
\setlength{\topmargin}{0in}
\setlength{\textheight}{8.5in}

\newcommand{\doctitle}{Low Precision Arithmetic for Convolutional Neural Network Inference}
\title{\doctitle}
\author{C. Clayton Violand\footnote{MSc Program in Cognitive Systems, Universit\"at Potsdam.}}

\begin{document}
\maketitle










\begin{abstract}
It is well-known in principle that the full precision of computer hardware (e.g. 32 or 64 bit floats) is not really needed for neural networks. By design they have to be robust against small perturbations in the data and also the activations of units in higher layers (i.e. dropout). Thus, there is a recent trend of moving to low-precision calculations in order to speed up inference in deep neural networks. In fact, the new nVidia GPUs coming out at the end of this year will feature an 8-bit integer mode specifically for inference in deep Neural Networks, which in theory runs 4x as fast as single precision floats. However, there are few studies on when exactly low precision is enough and how much speed this can bring. This thesis deals with the implementation of neural networks with different computing precisions, and how well they still work depending on the Neural Network architecture. It also explores the gain in speed obtainable on different architectures.
\end{abstract}










\section{Introduction}
Computation in convolutional neural networks require large matrix multiplication operations, with most of the heavy lifting being performed by the convolutional layer. In the case of the well-known Imagenet architecture, 89\% of CPU processing time is spent on convolutions \cite{gemm}. As such, these networks can be costly and are slow at inference time. Different methods exist to implement convolution: the GEMM method is a popular method and involves only one large matrix multiplication operation in the convolutional layer. Still, this operation can be slow due to the size of the matrices and the nature of matrix multiplication, which is intensive. 
One solution to this problem is to reduce the computation precision at the convolutional layer. This would speed up inference. However, it may also reduce performance accuracy. This paper deals with the trade-off between speed and performance accuracy in performing convolution with different computation precisions.










\section{Convolutional Neural Networks}
Convolutional neural networks are a type of neural network specifically made for image processing. Like normal neural networks, the computation involves "neurons" with weights and biases that are learnable through training. The weights and biases in each neuron are multiplied with some input matrix, and the dot products of each neuron in a particular layer is summed with the other neurons. The output is then passed on to a non-linear transformation function, and pushed forward in the pipeline of layers.

Images are large and don't work well with normal neural network architecture. An image of śmall size, perhaps 28x28 pixels and 3 channels (RGB), will already require a network with \begin{math}28*28*3 = 2352\end{math} weights for each neuron. Such a network would surely contain mutliple neurons as well. Therefore, it's clear to see how fully-connected neural networks don't scale well to image data.

Convolutional neural networks solve this through computation in a convolution layer. In such a layer, small weight matrices called kernels, which are a kind of filter, are multiplied with only a section of the input the size of the kernel itself. Several of these kernels iterate over the input in several dot product calculations but nevertheless drastically reduce the amount of parameters, as each iteration over the input is met by the same kernel. In other words, the weight parameters are \textit{recycled}.

This duplication of parameters is possible due to the inerhent nature of images. It can be said that a pattern that is useful in one section of the image might also be useful in another section of the image. Therefore, kernel weights are duplicated across dot-product iterations over the input.










\section{Timing Experiments}
The following timing experiments were performed on an \textit{Intel® Core™ i5-2520M CPU @ 2.50GHz × 4}. The command at compile time was: 

\textit{-c -O3 -march=native -std=c++11 -msse4.1 -lpthread}

There are two convolutions involved in our mnist model. These include one convolution at 576x20x25 (length, depth, width) and another convolution at 64x50x500.

There are three convolutions involved in our cifar model. These include one convolution at 1024x32x75, one convolution at 256x32x800, and a final convultion at 64x64x800.

The mean processing time over 100 images is reported for each convolution in Table \ref{table:1} on page \pageref{table:1}. As seen in Table \ref{table:2} on page \pageref{table:2}, one can see that there is a linear correlation between the total dimensional magnitude of the GEMM multiplication and the percentage increase in speed, with low dimensionalities (here 288,000) even being a detriment when using integer multiplication.

\begin{table}[h]
\centering
\caption{Processing Time in seconds (mean over 100 images)}
\label{table:1}
\begin{tabular}{lll}
                            & \textbf{Eigen}      & \textbf{Gemmlowp} \\
\textbf{MNIST}              &                     &                   \\
convolution\_1 @ 576x20x25  & \textbf{0.00006044} & 0.0000712         \\
convolution\_2 @ 64x50x500  & 0.00022997          & \textbf{0.000177} \\
\textbf{CIFAR}              &                     &                   \\
convolution\_1 @ 1024x32x75 & 0.00042064          & \textbf{0.000322} \\
convolution\_2 @ 256x32x800 & 0.00129486          & \textbf{0.000663} \\
convolution\_3 @ 64x64x800  & 0.00046174          & \textbf{0.00031} 
\end{tabular}
\end{table}

\begin{table}[h]
\centering
\caption{Advantage with Gemmlowp}
\label{table:2}
\begin{tabular}{ll}
Dimensionality     & Increase In speed using Gemmlowp \\
576x20x25=288000   & -15\%                            \\
64x50x500=1600000  & 23\%                             \\
1024x32x75=2457600 & 24\%                             \\
64x64x800=3276800  & 33\%                             \\
256x32x800=6553600 & 49\%                            
\end{tabular}
\end{table}









\clearpage
\begin{thebibliography}{9}

\bibitem{} 
http://cs231n.github.io/convolutional-networks/

\bibitem{} 
http://christopher5106.github.io/deep/learning/2015/09/04/Deep-learning-tutorial-on-Caffe-Technology.html

\bibitem{} 
https://github.com/BVLC/caffe/wiki/Model-Zoo

\bibitem{} 
http://stackoverflow.com/questions/39169012/extract-weights-of-network-in-caffe

\bibitem{} 
https://github.com/BVLC/caffe/issues/20

\bibitem{gemm} 
https://petewarden.com/2015/04/20/why-gemm-is-at-the-heart-of-deep-learning/

\bibitem{} 
http://nghiaho.com/?p=1726

\bibitem{} 
https://github.com/soumith/convnet-benchmarks

\bibitem{} 
http://static.googleusercontent.com/media/research.google.com/de//pubs/archive/37631.pdf

\bibitem{} 
https://petewarden.com/2016/05/03/how-to-quantize-neural-networks-with-tensorflow/

\bibitem{} 
https://github.com/google/gemmlowp

\end{thebibliography}










\end{document}