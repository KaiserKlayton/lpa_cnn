It is well-known in principle that the full precision of computer hardware (e.g. 32 or 64 bit floats) is not really needed for neural networks. By design they have to be robust against small perturbations in the data and also the activations of units in higher layers (i.e. dropout). Thus, there is a recent trend of moving to low-precision calculations in order to speed up inference in deep neural networks. In fact, the new nVidia GPUs coming out at the end of this year will feature an 8-bit integer mode specifically for inference in deep Neural Networks, which in theory runs 4x as fast as single precision floats. However, there are few studies on when exactly low precision is enough and how much speed this can bring. This thesis deals with the implementation of neural networks with different computing precisions, and how well they still work depending on the Neural Network architecture. It also explores the gain in speed obtainable on different architectures.