It is well-known in principle that the full precision of computer hardware (e.g. 32- or 64-bit floats) is not really needed for neural networks. By design, they are robust against phenomena like small perturbations in data, stochastic activations of units (e.g. dropout) and weight randomization. Thus, there is a recent trend of moving to low-precision calculations in order to speed up inference in deep neural networks. In fact, recently designed GPUs feature an 8-bit integer mode specifically tailored for inference in deep neural networks. This would in theory run inference at four times the speed of 32-bit floats. However, there are few studies on exactly how low-precision calculations affect classification accuracy and speed at inference time. This work deals with the implementation of several neural network architectures in C++, the analysis of how well they hold up given a change in arithmetic precision, and the creation of a robust benchmarking tool equipped to carry out such analyses.