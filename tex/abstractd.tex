Es ist allgemein bekannt, dass die volle Genauigkeit der gegebenen Computerhardware (32bit oder 64bit) für neuronale Netzwerke nicht benötig wird. Durch ihr Design sind sie robust gegenüber kleinen Störungen in den Daten, stochastischer Aktivierungen von einzelnen Einheiten, wie z.B. durch Dropout, und gegen die zufällige Änderung von Gewichten. Hierdurch ergibt sich ein aktueller Trend zur Berechnung mittels niedrigerer Genauigkeit, um eine bessere Geschwindigkeit der Berechnungsoperationen zu erreichen. Aktuell hergestellte Grafikkarten stellen einen 8bit Integer Modus bereit, der speziell für die Verarbeitung innerhalb von neuronalen Netzen angepasst ist. Theoretisch würde dies zu einer vierfachen Geschwindigkeitssteigerung gegenüber der 32bit Fließkommavariante führen. Jedoch gibt es einige Studien, die genau aufzeigen, welchen Einfluss eine niedrigere Genauigkeit bei der Berechnung auf die Ergebnisse bei Klassifikation hat und wie die Laufzeit beeinflusst wird. In dieser Arbeit werden Implementierungen verschiedener neuronaler Netzwerkarchitekturen in C++ beleuchtet. Weiterhin wird analysiert, welche Änderung im Ergebnis durch die Verringerung der Genauigkeit entsteht. Außerdem wird eine stabile Umgebung entwickelt um Benchmarks sicher zu erheben.