Es ist grundsätzlich bekannt, dass die volle Genauigkeit der Computerhardware (z. B. 32 oder 64 Bit Floats) für neuronale Netze nicht wirklich benötigt wird. Durch das Design müssen sie robust gegen kleine Störungen in den Daten und auch die Aktivierungen von Einheiten in höheren Schichten (d. H. Ausfall) sein. So gibt es einen jüngsten Trend, sich zu niedrigen Präzisionsberechnungen zu bewegen, um die Schlußfolgerung in tiefen neuronalen Netzwerken zu beschleunigen. In der Tat werden die neuen nVidia GPUs, die am Ende dieses Jahres herauskommen, einen 8-Bit-Integer-Modus speziell für die Schlußfolgerung in tiefen Neuronalen Netzwerken, die in der Theorie läuft 4x so schnell wie einzelne Präzision schwimmt. Allerdings gibt es nur wenige Studien, wann genau genaue Präzision genug ist und wieviel Geschwindigkeit das bringen kann. Diese Arbeit beschäftigt sich mit der Implementierung von neuronalen Netzwerken mit unterschiedlichen Rechenpräzisionen und wie gut sie je nach Neural Network Architektur arbeiten. Es erforscht auch den Gewinn an Geschwindigkeit, der auf verschiedenen Architekturen erhältlich ist.