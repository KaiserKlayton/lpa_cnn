\chapter{Introduction}

\section{Background}
    
%file:///home/clayton/Documents/cnn/cnns_1.html

Computations in convolutional neural networks require large matrix multiplication operations, with most of the heavy lifting being performed by the convolutional layer. In the case of the well-known Imagenet architecture, 89\% of CPU processing time is spent on convolutions. As such, these networks can be costly and are slow at inference time. Different methods exist to implement convolution: the GEMM method is a popular method and involves only one large matrix multiplication operation per convolutional layer. Still, this operation can be slow due to the size of the matrices and the intensive nature of matrix multiplication. One solution to this problem is to reduce the computation precision at the convolutional layer, thus speeding up inference. However, this adjustment may also reduce performance accuracy. This paper deals with the trade-off between speed and performance accuracy as a result of performing integer multiplication for convolutions with GEMM.

Convolutional neural networks are a type of neural network specifically made for image processing. Like normal neural networks, the computation involves "neurons" with weights and biases that are learnable through training. The weights and biases in each neuron are multiplied with some input matrix, and the dot products of each neuron in a particular layer is summed with the other neurons. The output is then passed on to a non-linear transformation function, and pushed forward in the pipeline of layers.

Images are large and don't work well with normal neural network architecture. An image of Å›mall size, perhaps 28x28 pixels and 3 channels (RGB), will already require a network with 28*28*3 = 2352 weights for each neuron. Such a network would surely contain mutliple neurons as well. Therefore, it's clear to see how fully-connected neural networks don't scale well to image data.

Convolutional neural networks solve this through computation in a convolution layer. In such a layer, small weight matrices called kernels, which are a kind of filter, are multiplied with only a section of the input the size of the kernel itself. Several of these kernels iterate over the input in several dot product calculations but nevertheless drastically reduce the amount of parameters, as each iteration over the input is met by the same kernel. In other words, the weight parameters are recycled.

This duplication of parameters is possible due to the inerhent nature of images. It can be said that a pattern that is useful in one section of the image might also be useful in another section of the image. Therefore, kernel weights are duplicated across dot-product iterations over the input.

\section{Problem definition}
\section{Summary of findings}
\subsection{Speed}

The mean processing time over 100 images is reported for each convolution in Table on page. As seen in Table on page , one can see that there is a linear correlation between the total dimensional magnitude of the GEMM multiplication and the percentage increase in speed, with low dimensionalities (here 288,000) even being a detriment when using integer multiplication.

\subsection{Accuracy}

\section{Definition of terms}
\section{Review of literature}
\section{Chapter outline}