\chapter{Introduction}

Computations in convolutional neural networks (CNNs) require large matrix multiplication operations, with most of the heavy lifting being performed by the convolution layer. In the case of the well-known AlexNet architecture, 89\% of CPU processing time is spent on convolutions \cite{warden_gemm}. As such, these networks can be costly and are slow at inference time. Different methods exist to implement convolution. The GEMM method is a popular method and involves only one large matrix multiplication operation per convolution layer. Still, this operation can be slow due to the size of the matrices and the intensive nature of matrix multiplication. One solution to this problem is to reduce the computation precision at the convolution layer, thus speeding up inference. However, this adjustment may also reduce performance accuracy. This paper deals with the trade-off between speed and performance accuracy as a result of performing integer multiplication for convolutions with GEMM, and the creation of a framework suitable for testing these performance differences.

All neural network computations involve ``neurons'', or nodes, with weights and biases that are learnable through training. In simple feed-forward neural networks such as single- or multi-layer perceptrons, the layers are ``fully connected'', meaning the weights in each layer are multiplied with \textit{each} node of the input matrix, then all products in a particular layer are summed together. The output is then passed on to a non-linear transformation function and pushed forward in the pipeline of layers. The perceptron feed-forward neural network represents the most basic neural network, and was the first devised \cite{perceptron}.

Images are large and don't work well with fully-connected perceptron neural network architectures. An image of small size, perhaps $28\times28$ pixels and three channels (RGB), will already require a network with $28\times28\times3 = 2352$ weights for each neuron. Such a network would surely contain multiple neurons as well. Therefore, it's clear that fully-connected neural networks don't scale well to image data. 

Convolutional neural networks are advantageous because they reduce the amount of parameters needed, and exploit ``pixel neighborhoods''. The convolution layer forms the foundation of convolutional neural networks. In such a layer, small weight matrices---or kernels---acting as a kind of filter, are multiplied with only a section of the input the size of the kernel itself. A predetermined number of these kernels iterate over the input in several matrix product calculations but nevertheless drastically reduce the amount of parameters, as each iteration over the input is met by the same kernel. In other words, the weight parameters are recycled. This duplication of parameters is possible due to the inherent nature of images. It can be said that a pattern that is useful in one section of the image might also be useful in another section of the image: this is what is meant by ``exploitation of pixel neighborhoods''. In this way, kernel weights are duplicated across product iterations over the input.

However, these networks are large. Recently, researchers have been moving towards quantization to enable faster inference times for these networks \cite{warden_quantize}. Quantization involves a translation of floating-point representations to integers, thus reducing computational resource needs. In theory, quantization could speed up inference by a factor of four. However, in practice, quantization is met with its own challenges. Firstly, each quantized convolution takes several parameters. Current techniques suggest a calculation of these parameters ``offline'' as much as possible. This work will show how important this offline calculation really is when it comes to reducing inference time. Secondly, quantization itself can slow down inference when not implemented in an optimal way. This, too, will be shown to be costly and a worthy candidate for optimization.

Many CNN architectures have been trained to address the image recognition problem, making great strides in reducing memory consumption and speed at both training and inference time. Some seminal networks include those born out of the Imagenet Large Scale Visual Recognition Challenge, or ILSVRC, trained on a popular dataset of the same name, drawing from a database of over ten million hand-annotated images \cite{imagenet}. The VGG family of models from Oxford University \cite{return} are one example, as are Deep Residual Networks, winning first-place at the ILSVRC competition in 2015 \cite{resnets2}. Deep Residual Networks, or ``ResNets'', operate by intertwining layers in a non-linear fashion, and happen to benefit greatly from increased network depth. They have significantly reduced parameter sizes, however, and thus speed up inference. For example, while the 16- and 19-layer VGG neural networks require 15.3 and 19.6 billion FLOPS (floating-point operations)---or multiply-add operations---respectively, the 151 layer ResNet151 only requires 11.3 billion FLOPS \cite{resnets2}. Nonetheless, the amount of convolutions in one network have risen as high as 154, thus maintaining the importance of optimization of the convolution at inference time. 

Despite the advantages of residual neural networks, this work will show that the low-precision arithmetic technique fails for ResNets (and for not very well-established reasons). The situation calls for further investigation, as the potential of these innovative networks has shown to be immense \cite{resnets1}.

This work is organized as follows. In Chapter 2, a background on the various processes and technologies involved in this research are outlined. The reader will be familiarized with core concepts, namely those that contribute to the workings of convolution and low precision in neural networks. 

In Chapter 3, the experimental methodology is given. The processes involved for setting up the experiments and for designing the elements of the system are explained.

Chapter 4 gives a detailed account of the experimental results in regards to changes in accuracies and speeds in the context of different types of arithmetic precision. Results from auxiliary experiments on an embedded system, a Raspberry Pi 3, are also given.

Chapter 5 serves as a conclusion, and will mention both system and experimental limitations, as well as items which were found to be of most immediate importance in regards to future research.