\chapter{Methodology}

In this chapter, the experiment's methodology is summarized and abstracted in regards to the research goals, experimental setup and program implementation of the work. The process of preparing and benchmarking neural networks with varying arithmetic precisions, as well as the creation of an end-to-end system for carrying out these tests, is detailed. In brief, trained models are obtained from the scientific community for each architecture form various resources, model weights are extracted and inference calculations are made at different precisions.

\section{Experimental summary}
The goal of this work is to study how performing inference calculations with reduced arithmetic precision affects two measures: runtime and accuracy. Given a convolutional neural network model $M$, runtime $R$ and accuracy $A$ of inference with $M$ are calculated with a varying GEMM mode $C \in{eigen, gemmlowp}$ where $eigen$ \cite{eigen} uses 32-bit floating-point representation and $gemmlowp$ \cite{gemmlowp} uses quantized 8-bit integers. $gemmlowp$ also performs quantization, the process of which is detailed in section \ref{sec:quantization}. A reference implementation using Caffe \cite{caffe} was implemented, as described in section \ref{sec:caffe}.

Concretely, formula \ref{eqn:conv} on page \pageref{eqn:conv} is either computed in full float precision or with quantized values as described in formulas \ref{eqn:scale}, \ref{eqn:zero}, \ref{eqn:convert}, \ref{eqn:clamp} in section \ref{sec:quantization}.

\section{Model architectures}
The models used in this work were chosen because they represent small, medium and large model sizes in terms of *the number of trainable weight parameters*. Additionally, in their number of convolutions, they trend upwards, generally exponentially, which creates a natural environment for the comparison of runtimes of convolutions (see figure \ref{fig:trend}. ``LeNet'' \cite{mnist} and ``cifar-10\_quick'' \cite{cifar} represent the two smaller models. The larger models include the popular ``VGG-16'' and the even larger ``VGG-19'' \cite{return}. The medium-sized models included in experimentation consist of the revolutionary ``ResNet50'' and ``ResNet101'' \cite{resnets2} models. For the purpose of the experiments the convolution layers are of most interest. Each model has a different number of these layers. LeNet has a mere two such layers, cifar-10\_quick has three, and VGG-16 and VGG-19 have 13 and 16 convolution layers, respectively. Refer to table \ref{tbl:cnns} on page \pageref{tbl:cnns} for the layer outline of these model architectures. ResNet50 and ResNet101 have---by far---the most number of layers, totaling 53 such operations for ResNet50, and 104 operations for ResNet101.

\bildklein{figures/trend.png}{Convolution counts}{The relationship between convolution counts for the models used.}{fig:trend}

The model architectures and their relevant properties are highlighted in \ref{tbl:models} on page \pageref{tbl:models}, while some diagrams of varying model designs are displayed below in figures \ref{fig:vgg16_design} (page \pageref {fig:vgg16_design} and \ref{fig:resnet_design} (page \pageref{fig:resnet_design})).

\bildgross{figures/vgg16.png}{Design of the VGG-16 architecture}{Design of the VGG-16 architecture. [Image courtesy of book.paddlepaddle.org]}{fig:vgg16_design}

\bildgross{figures/resnet.png}{Design of the ResNet architecture}{Design of the ResNet architecture. [Image courtesy of book.paddlepaddle.org]}{fig:resnet_design}

\begin{table}[]
\centering
\caption[Model properties]{The models and their properties.}
\label{tbl:models}
\begin{tabular}{lllllll}
\textbf{Model}     & \textbf{layers} & \textbf{params} & \textbf{input} & \textbf{convs} & \textbf{pools} & \textbf{products} \\
LeNet     & 4               & 430.5K          & 1x28x28        & 2              & 2              & 2                 \\
cifar-10  & 5               & 145.4K          & 3x32x32        & 3              & 3              & 2                 \\
VGG-16    & 16              & 138.3M          & 3x224x224      & 13             & 5              & 3                 \\
VGG-19    & 19              & 143.7M          & 3x224x224      & 16             & 5              & 3                 \\
ResNet50  & 50              & 25.6M           & 3x224x224      & 53             & 2              & 1                 \\
ResNet101 & 101             & 44.5M           & 3x224x224      & 104            & 4              & 1                
\end{tabular}
\end{table}

\section{Inference inputs}
The size and nature of inputs also differs amongst models used. The LeNet models uses its famous MNIST handwritten database dataset of hand-written digits \cite{mnist}. These are one-channel (gayscale) images of size $28\times28$. The cifar-10\_quick model is fed with $32\times32$ 3-channel (RGB) images. They are mean-image normalized. All four other models take ImageNet images as input, specifically the first 1000 images of the ILSVRC 2012 competition's validation set \cite{imagenet}. They are either mean-pixel or mean-image normalized, depending on what was done at training-time, and are of size $224\times224$.

\section{Breaking down the low-precision process}
The requirements for integrating low precision into convolutions can be broken down into several components. Some of these can---and are---obtained or performed offline, while others are strictly online run-time processes. These components are, briefly: result matrix quantization parameters, input quantization parameters, weight quantization parameters, the actual quantization process for inputs, activations and weights, the GEMM operation, and dequantization. Figure \ref{tbl:gemmlowp} on page \pageref{tbl:gemmlowp} shows the different steps involved, whether they can be performed offline, and whether they are involved in calculating timings for the experiments. The quantization method is highlighted in section \ref{sec:quantization}.

\section{Implementation}
\subsection{Caffe feature and weight extraction}
\label{sec:caffe}
Caffe is a popular deep-learning framework born out of Berkeley College \cite{caffe}. It has a vast number of publicly-accessible models made available, and processes in 32-bit float \cite{caffe}, thus making it a good candidate for building in to our custom system as a means to select models and provide model weights and baseline features.

At this point, it may also be of use to keep track of the number representations of the various libraries involved in extracting weights and features, as to be able to effectively compare to the Caffe baseline, and to keep our weights in the right precision.

Weights are extracted from the forward-pass of the Caffe network, instantiated through Caffe's ``Caffe.Net()'' method, taking a .prototxt and .caffemodel file and called with ``Caffe.Net.().forward()'' \cite{caffe}. Caffe processes in single floating-point precision, so the current state is 32-bit. After extraction, weights are saved via Python's Numpy package, which uses double precision \cite{scipy}, so our precision is maintained.

First in the pipeline of the Caffe feature extraction is, after the forward pass, the saving of activations by Python's Numpy \cite{scipy}. Therefore the reference features are of the correct precision and are comparable to the 32-bit experiment results.

The inputs, having values typically ranging from 0-255, are inherently integers and remain so until activated in the first layer of the network (or rather are represented by floats with a trailing ``.0'', unless quantized), where they either remain integers (in integer mode) or become floats due to multiplication with weights.

\subsection{Inference}
All experiments were run on an Intel Core i5-2520M CPU @ 2.50GHz x 4 with 3.7 GB of RAM. The operating system was Ubuntu 16.04 LTS 64-bit. Additionally, auxiliary experiments were run on a Raspberry Pi 3 (see section \ref{sec:pi}.

In both modes, input is run through the custom system related to this work. The system includes bare-bones implementations of all relevant layers: convolution, pooling, ReLU, fully-connected, batch-normalization, scale, and eltwise. The inference files and makefiles are automatically generated via a script-generation script written in Python, which parses the Caffe prototxt files and outputs a C++ script in the most minimal way possible. Great care has been taken in ensuring that the script is minimal: all large objects are passed-by-reference when possible, eigen routines are called conservatively, the loading of parameters, weights and inputs, handled by the Armadillo package \cite{eigen_vs_armadillo}, is optimized, and the compiler is set to optimize for speed. The options used at compile time are as follows: 

\lstset{language=make}
-c -O3 -march=native -std=c++11

The option ``-O3'' sets the compiler to make full optimization: it attempts to reduce code size and execution time, and performs all other optimizations possible. Furthermore, ``-march=native'' tells the compiler to use the platform-specific assembly code instructions.

\subsection{Integrating low precision}
The low-arithmetic precision module was taken from gemmlowp, a ``small self-contained low-precision GEMM library'' \cite{gemmlowp}. Gemmlowp performs several tasks needed for the experiments in this work. Apart from performing the actual GEMM procedure, gemmlowp quantizes inputs, dequantizes, retrieves parameters needed for these quantizations, and quantizes weights. This section will detail the processes involved in replacing regular floating-point GEMM operations with a custom call to the gemmlowp module.

\begin{table}[]
\centering
\caption[Low-precision GEMM broken down by process]{Subprocesses of low-precision GEMM. Although low-precision arithmetic is in theory faster, computation time can quickly add up with other online operations.}
\label{tbl:gemmlowp}
\begin{tabular}{lll}
\textbf{Process}                                       & \textbf{possible offline?} & \textbf{incl. in timing?} \\
Calculate input matrix MIN/ MAX     & yes                        & yes                       \\
Calculate weight matrix MIN/ MAX     & yes                        & no                        \\
Calculate activation matrix MIN/ MAX & can estimate               & no                        \\
Quantize input matrix                & yes                        & yes                       \\
Quantize weight matrix               & yes                        & no                        \\
GEMM                                 & no                         & yes                       \\
Dequantize activation matrix         & no                         & yes                      
\end{tabular}
\end{table}

One of the convenient realities that makes low-precision GEMM work is the fact that many---and sometimes all---of the parameters can be calculated offline \cite{warden_quantize}. For the weights of a cnn, this is always the case, as the weight parameters are learned at training-time. Thus, the minimum and maximum of these matrices can be stored as constants. For the inputs, it is also possible to collect parameters offline. This is straightforward for standard RGB images, known to hold values in the range [0-255]. The exact processes that accompany these calculations are detailed in section \ref{sec:quantization}, with equations \ref{eqn:scale}, \ref{eqn:zero}, \ref{eqn:convert} and \ref{eqn:clamp}.

For result/ activation parameters, this is a bit more complex. One cannot know beforehand the resulting matrices at each convolution. However, it's possible to collect estimations of these parameters using the training images. In this experiment, these parameters are gather beforehand and passed to the function call. This cuts down on timing and apparently doesn't cost much in terms of accuracy, as is explained in the next chapter.

Quantizing inputs and weights, like finding their parameters, can also be done offline. However here, quantization parameters and quantizations of the input layer have been treated equally with other activation layers, and thus are computed online. We assume for the purposes of these experiments that the input matrix range was not known beforehand.

GEMM itself is an online operation, as well as dequantizing activation matrices, as results are of course not known until after GEMM.