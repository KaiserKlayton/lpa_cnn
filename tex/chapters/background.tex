\chapter{Background and past work}

This chapter will familiarize the reader with the various topics, strategies and technologies involved in this research. Namely, convolutional neural networks, convolutions, computer precision and integer representation, and other relevant topics will be discussed.

\section{Image recognition}
Image recognition is a sub-task in the field of computer vision in which the computer's goal is to correctly predict a text label for the most dominant object in the image \cite{history}. Convolutional Neural networks are designed to handle the image recognition problem. They got their start at the ILSVRC challenege in 2012, where AlexNet, the first CNN, won by a significant factor \cite{history}. Since then, convolutional neural network architectures have been modified and experimented with, keeping the core concept of the convolutional layer in mind. As a result of this research, there now exist CNNs which perform at near-human levels. One example is the popular lenet architecture \cite{mnist}, where top-5 accuracy for the task of recognizing hand-written numbers has reached 100\%. Another example is the family of residual neural networks, whos very deep architectures and interwoven stacks of convolutional layers have achieved an error of only 3.57\% on the famous Imagenet dataset \cite{resnets2}.

\section{Convolutional neural networks}
\subsection{The limitations of non-convolutional neural networks}
In a traditional neural network, each node in each layer is fully-connected to each node in the preceding layer. This means that every activation is derived from the full scope of input from the previous layer, making the process computationaly expensive and slow. The memory needed for such a network is staggering. Imagine a modestly-sized network that takes images of size $224\times224$ and 3 channels (RGB), with perhaps only two hidden layers comprising of 4096 and 1000 nodes, sequentially. All together this network would require $224\times224\times3 = ~150K$ values for the input layer, ~4K  and 1K values to represent the first and second hidden layers, with $224\times224\times3\times4096 = ~616M$ and $4096\times1000 = ~4M$ values in weights for these hidden layers, respectively. This would amount to a total of $775M\times4 = 3,1 GB$ of storage needed---at floating point precision---for this relatively simple network. It's also of note that in such a scheme in which there is such a large amount of independant and individual parameters, it would be difficult not to over-fit the data at training time. One can easily see that the traditional non-convolutional network is not ideal for image input.

\bildklein{figures/nn.png}{A traditional neural network}{A traditional neural network. Notice how each node is connected to every previous node. Weights add up quickly and the image-recognition problem soon becomes unmanageable. [CC Image courtesy of Chrislb on Wikimedia Commons]}\label{fig:nn}

\subsection{Convolutional neural networks as a solution}
Convolutional neural networks approach the image recognition problem differently. Namely, they make one main assumption and one crucial adjustment. The assumption made is that the input is an image, and nothing else. The adjustment is the introduction of parameter recycling and the replacement of fully-connected layers with convolutions. Because of the nature of images, sharing of parameters across the network is possible, thus reducing their number drastically. By expecting image input, and by recognizing the regular ``nature of images'' to be consistant, one can build in pre-meditated adjustments to the network through various techniques \cite{convnets}. One clear way in which this works is by recognizing that one section of an image may share properties with another section of the image. It's possible in such a situation to replace the fully-connected ``filter'' with local and smaller filters (e.g. $3\times3$), only looking at one section of the image at a time. This filter can then be moved, *with the same weight parameters*, to other sections of the image, keeping in mind that if the filter works at one location in the image, it should also be applicable at another location. This, again, is realizable due to the ``nature of images'': namely that they share properties throughout their field \cite{convnets}. Replacing our first fully-connected layer in our previous example with a reasonable convolution layer with, say, 64 outputs and a filter-size of $3\times3$ would result in a weight parameter matrix of only size $3\times3\times3\times64 = ~1.7K$.

\bildgross{figures/cnn.png}{A basic convolutional neural network}{A basic convolutional neural network. Weight parameters are stored only for a pre-determined amount of filters (here, ``f.maps'') of a certain dimension. For example, shown are 5 filters of size $5\times5$ for one convolution, followed by 12 filters of size $3\times3$ for the second). [CC Image courtesy of Aphex34 on Wikimedia Commons]}\label{fig:cnn}

\subsection{Architectures}
Convolutional neural network architectures, although diverse, tend to vary in systematic ways. In their most basic form, CNNs may be comprised of only a few types of layers: the input layer, the convolution, the pooling layer, activation layers (e.g. ReLU), and the fully-connected layer \cite{convnets}. Typically, most of the heavy lifting in terms of computational resources will be---by a strong margin---the convolutional layer. Famous convolutional neural network architectures include AlexNet, GooglNet, ResNet and VGG, all of which differ in layer number, layer ordering, and layer parameters, such as the number of convolution filters, the size of filter, and the stride of the filter. These layers are arranged in varying fashion to make up the skeletons of the well-known convolutional neural network. To see some typical architectures outlined, see table \ref{tbl:cnns} on page \pageref{tbl:cnns}.

\begin{table}[]
\centering
\caption[Some well-known cnn architectures]{The basic outline of some well-known cnn architectures. Notice the variance in number of convolutions and placement of repeating stacks.}
\label{tbl:cnns}
\begin{tabular}{|l|l|l|l|l}
\cline{1-4}
\multicolumn{4}{|l|}{(ReLU layers omitted)} &  \\ \cline{1-4}
\textbf{LeNet}   & \textbf{Cifar-10}      	& \textbf{VGG-16}         & \textbf{VGG-19}         &  \\ \cline{1-4}
conv             & conv     				& conv                    & conv                    &  \\ \cline{1-4}
pool             & pool     				& conv                    & conv                    &  \\ \cline{1-4}
conv             & conv     				& pool                    & pool                    &  \\ \cline{1-4}
pool             & pool  				    & conv                    & conv                    &  \\ \cline{1-4}
fc               & conv   				    & conv                    & conv                    &  \\ \cline{1-4}
fc               & pool     				& pool                    & pool                    &  \\ \cline{1-4}
                 & fc     					& conv                    & conv                    &  \\ \cline{1-4}
                 & fc      					& conv                    & conv                    &  \\ \cline{1-4}
                &       						& conv                    & conv                    &  \\ \cline{1-4}
                &       						& pool                    & conv                    &  \\ \cline{1-4}
                &      						& conv                    & pool                    &  \\ \cline{1-4}
                &       						& conv                    & conv                    &  \\ \cline{1-4}
                &       						& conv                    & conv                    &  \\ \cline{1-4}
                &      						& pool                    & conv                    &  \\ \cline{1-4}
                &       						& conv                    & conv                    &  \\ \cline{1-4}
                &       						& conv                    & pool                    &  \\ \cline{1-4}
                &       						& conv                    & conv                    &  \\ \cline{1-4}
                &       						& pool                    & conv                    &  \\ \cline{1-4}
                &       						& fc                      & conv                    &  \\ \cline{1-4}
                &       						& fc                      & conv                    &  \\ \cline{1-4}
                &       						& fc                      & pool                    &  \\ \cline{1-4}
                &       						&                         & fc                      &  \\ \cline{1-4}
                &       						&                         & fc                      &  \\ \cline{1-4}
                &       						&                         & fc                      &  \\ \cline{1-4}
\end{tabular}
\end{table}

\section{Convolution}
The convolution is the cornerstone of the convolutional neural network, and can be said to be made up of several components. The first component is the filter, or kernel. The kernel is of a pre-determined dimension (usually small, e.g. $3\times3$ or perhaps $7\times7$) and reaches through the entire depth of the input. Its job is to slide across the input at a pre-determined ``stride'', much like a moving window, multiplying itself with the current input lying within this window \cite{convnets}.

The second component can be said to be the hyperparameter of depth. This does not refer to the depth of the input volume, but rather the number of kernels used. There are typically several filters, and each of these filters performs its task as described above, delivering a two-dimensional matrix product result.

The third component of the convolution is the input itself. The input is three-dimensional and is passed from the preceeding layer at inference to the convolution operation.

The remaining components make up what can be viewed as the hyperparameters of the kernel, and include its shape, its stride and the amount, if any, of padding it adds to the input layer. A typical size is $3\times3$, and a typical stride is 1.

\bildklein{figures/convlayer.png}{The convolution kernel at work}{Each ``set'' of kernels (here, of count 5) focuses on a small section of the input (in pink) at a time, reaching through its entire depth. The result from each of these sets of 3-dimensional kernels is a stack of two-dimensional activations, making up one three-dimensional section (small rectangle in blue) of the total output volume (large box in blue), having the depth of the number of kernels in the set. This operation is repeated all over the image. [CC Image courtesy of Aphex34 on Wikimedia Commons]}\label{fig:convlayer}

It will now be shown in the next section how the convolution operation can be made more efficient.

\subsection{GEMM}
As previously mentioned, in the case of the famous AlexNet architecture, 89\% of computational processing time is taken up by convolutions. Thus, it is a worthy undertaking to optimize the mathematical operation itself, making it as efficient as possible for the computer. The agreed upon solution is the GEMM operation, or General Matrix to Matrix Multiplication, dating back to 1979 and described in ``Basic Linear Algebra Subprograms for Fortran Usage,'' \cite{BLAS}. BLAS improves upon the normal convolution operation by transforming it into one single matrix-matrix multiplication. Normally, a convolution involves the process described in figure \ref{fig:convlayer} on page \pageref{fig:convlayer}, where a filter performs tiny matrix-matrix multiplications all over the image. Instead, GEMM performs the ``image-to-column'' operation (commonly known as ``im2col'') \cite{im2col}, succesffully translating the input volume and weight kernels such that the result of a convolution involves one and only one matrix multiplication between the im2col output of input and the im2col output of weights.

Behind the scenes, im2col is simply serializing each input selection (small pink box in figure \ref{fig:convlayer} on page \pageref{fig:convlayer}) into one row of the new input matrix. Similarly, each kernel is serialized as a column of the new weight matrix. The result is now a classic matrix-matrix multiplication, visualized in figure \ref{fig:mm} on page \pageref{fig:mm}.

\bildklein{figures/matrixmult.png}{Matrix multiplication}{After GEMM prepares the input and weight volumes, the result of a convolution is obtained simply through traditional matrix-matrix multiplication. [CC Image courtesy of Quartl on Wikimedia Commons]}\label{fig:mm}

In the next section, now with a firm understanding of the inner workings of convolutional neural networks and the convolution operation itself, quantization, as a method to reduce computational overhead and increase inference speed, is discussed.

\section{Quantization}
\subsection{Computer number representation}
In order to understand why quantization is important, it would be beneficial to review how the computer stores numbers.

32-bit floating-point representation is a computer representation of real numbers. Although an estimation in itself---as real numbers are infinite and computer-representations cannot be---they offer a certain degree of precision in number representation and calculations \cite{ieee}. In a computer, they take scientific notation form (see figure \ref{eqn:floatrep} on \pageref{eqn:floatrep}) and can represent $2^{32}$ individual values. They use 1 bit for the sign, 8 bits for the exponent, and 23 bits for the fraction \cite{ieee}.

\eqn{-9.876 = \overbrace{\underbrace{-1}_\text{sign}}^\text{1 bit}\times\overbrace{\underbrace{9876}_\text{fraction}}^\text{23 bits}\times\overbrace{\underbrace{2^{-3}}_\text{exponent}}^\text{8 bits}}{Floating-point number representation}{Floating-point number representation. Notice how many bits it requires to represent one value in memory. This quickly adds up in convolutional neural networks}\label{eqn:floatrep}

Integers, on the other hand, are representd by a fixed amount of bits, such as 8 \cite{ieee}. 32-bit/64-bit processors are able to access large chunks of memory at a time, and it would in theory be faster to use integers rather than floating point, as $\times4$ the amount of numbers would be accessed within a single memory-retrieval operation, reducing memory bandwidth by 25\%. However, 8-bit integers have the potential to represent only $2^{8}$ distinct numbers, and thus using them comes at a precision cost. It will be shown, despite this significantly reduced precision, That using integer representation in convolution calculations is still successfull.

\subsection{Quantization and its challenges}
In short, quantization is a conversion of floating-point representation, specifically 32 bit floating-point, to integer representation, or 8-bit fixed-point. The process of quantization is relatively straightforward. Taking the minimum and maximum of the floating-point representation, a new range is defined using an appropriate integer representation, such as 0 to 255. \cite{warden_quantize}. In other words, 0 will now represent the minimum value from the original unquantized matrix and 255 the maximum value (see table \ref{tbl:quantize} on page \pageref{tbl:quantize})

After quantization, calculations are performed as usual. The result is then converted back into 32-bit float using more minimum and maximum parameters and passed along the network.

\begin{table}[]
\centering
\caption{Quantizing 32-bit floats}
\label{tbl:quantize}
\begin{tabular}{|l|l|l}
\cline{1-2}
\multicolumn{2}{|l|}{Quantized value representation} &  \\ \cline{1-2}
\textbf{32-bit}           & \textbf{8-bit}           &  \\ \cline{1-2}
-2.356                    & 0                        &  \\ \cline{1-2}
1.201                     & 127                      &  \\ \cline{1-2}
4.758                     & 255                      &  \\ \cline{1-2}
\end{tabular}
\end{table}

It may be apparant to the reader at this point that there is not insignificant overhead involved in this process: namely, determining parameters and converting back and forth between quantized and dequantized representations. This will be addressed later in this work.