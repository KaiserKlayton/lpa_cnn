\chapter{Background and past work}

This chapter will familiarize the reader with the various topics, strategies and technologies involved in this research. Namely, convolutional neural networks, convolutions, computer precision and integer representation, and other relevant topics will be discussed.

\section{Past work}
Previous work addressing the neural network number-representation problem has shown quantizing values to integers, or in some cases only as far as fixed-point representation, to be effective. One such example is research born out of the Watson Research Center and Almaden Research Center \cite{gupta}. This work shows that it is possible to reduce representation to 16-bit not only at inference time, but at training time, and without incurring a loss in accuracy. This furthers the case for the robustness of convolutional neural networks and highlights the potential for quantization of networks at inference. A second example is a work in which weights are kept in floating-point representation while activations are converted to fixed-point representation. This paper makes the claim that number representation range (the ability to represent large and small values) is more important than representation precision. \cite{dblp}. Google has also made great strides in regards to the quantization topic. One work emphasizes the real-time intensity of neural network processing through the example of speech recognition, and succeeds in optimizing performance of such processing at runtime on CPUs \cite{google}.

\section{Image recognition}
Image recognition is a subtask in the field of computer vision in which the goal is to correctly predict a text label for the most dominant object in the image \cite{history}. Convolutional neural networks are designed to handle the image recognition problem. They got their start at the ILSVRC challenege in 2012, where AlexNet, the first CNN, won by a significant factor \cite{history}. Since then, convolutional neural network architectures have been modified and experimented with, keeping the core concept of the convolution layer in mind. As a result of this research, there now exist CNNs which perform at near-human levels. One example is the popular LeNet architecture \cite{mnist}, where top-5 accuracy for the task of recognizing hand-written numbers has reached 100\%. Another example is the family of residual neural networks, whose very deep architectures and interwoven stacks of convolution layers have achieved an error of only 3.57\% on the famous Imagenet dataset \cite{resnets2}.

\section{Convolutional neural networks}
\subsection{Motivation}
\subsubsection{The limitations of non-convolutional neural networks}
In a traditional neural network, each node in each layer is fully-connected to each node in the preceding layer. This means that every activation is derived from the full scope of input from the previous layer, making the process computationally expensive and therefore slow. The memory needed for such a network is staggering. Imagine a modestly-sized network that takes images of size $224\times224$ and three channels (RGB), with perhaps only two hidden layers comprising of 4096 and 1000 nodes, sequentially. All together this network would require $224\times224\times3 \approx 150K$ values for the input layer, \~4K and 1K values to represent the first and second hidden layers, with $224\times224\times3\times4096 \approx 616M$ and $4096\times1000 \approx 4M$ values in weights for these hidden layers, respectively. This would amount to a total of $775M\times4 \approx 3 GB$ of storage needed---at floating-point precision---for this relatively simple network. It is also of note that in such a scheme in which there is a large amount of independent and individual parameters, it would be difficult not to overfit the data at training time. One can easily see that the traditional non-convolutional network is not ideal for image input. Although large CNN architectures such as VGG can reach a similar number of parameters, they are at the same time much deeper and expressive. In the case of VGG-19, the fully-connected layer (usually occurring at the end of a CNN network) brings the total parameter count to approximately 138.3M \cite{return}. However, a similar multi-layer perceptron network of similar layer-size would far exceed this number.

\bildklein{figures/nn.png}{A traditional neural network}{A traditional neural network. Notice how each node is connected to every previous node. Weights add up quickly and the image recognition problem soon becomes unmanageable. [CC Image courtesy of Chrislb on Wikimedia Commons]}{fig:nn}

\subsubsection{Convolutional neural networks as a solution}
Convolutional neural networks approach the image recognition problem differently. Namely, they make one main assumption and one crucial adjustment. These networks assume that the input is an image, and nothing else. They then introduce parameter recycling and the replacement of fully-connected layers with convolutions. Because of the nature of images, sharing of parameters across the network is possible, reducing their number drastically. By expecting image input, and by recognizing the regular ``nature of images'' to be consistent, one can build in premeditated adjustments to the network through various techniques \cite{convnets}. To understand why such an assumption works, one can consider how one section of an image may share properties with another section of the image. It is possible in such a situation to replace the fully-connected ``filter'' with local and smaller filters (e.g. $3\times3$), only looking at one section of the image at a time. This filter can then be moved, *with the same weight parameters*, to other sections of the image, keeping in mind that if the filter works at one location in the image it should also be applicable at another location. This again is realizable due to the aforementioned ``nature of images'': namely that they share properties throughout their field \cite{convnets}. Replacing our first fully-connected layer in our previous example with a reasonable convolution layer with, say, 64 outputs and a filter-size of $3\times3$ would result in a weight parameter matrix of only size $3\times3\times3\times64 \approx 1.7K$.

\bildgross{figures/cnn.png}{A basic convolutional neural network}{A basic convolutional neural network. Weight parameters are stored only for a predetermined amount of filters (here, ``f.maps'') of a certain dimension. For example, shown are five filters of size $5\times5$ for one convolution, followed by twelve filters of size $3\times3$ for the second). [CC Image courtesy of Aphex34 on Wikimedia Commons]}{fig:cnn}

\subsection{Architecture}
\subsubsection{Overview}
Popular convolutional neural network models, although diverse, tend to vary in systematic ways. In their most basic form, CNNs may be comprised of only a few types of layers: the input layer, the convolution, the pooling layer, activation layers (e.g. ReLU), and the fully-connected layer \cite{convnets}. Typically, most of the heavy lifting in terms of computational resources will be, by a strong margin, performed by the convolutional layers. Famous convolutional neural network architectures include AlexNet, GooglNet, ResNet and VGG, all of which differ in layer number, layer ordering, and convolution layer parameters, such as the number of filters, the size of the filters, and the stride of the filters. These layers are arranged in varying fashion to make up the skeletons of the well-known convolutional neural networks. To see some typical architectures outlined, refer to Table \ref{tbl:cnns} on page \pageref{tbl:cnns}.

\begin{table}[]
\centering
\caption[Some well-known CNN architectures]{The basic outline of some well-known CNN architectures.}
\label{tbl:cnns}
\begin{tabular}{llll}
\multicolumn{4}{l}{(ReLU layers omitted)}                              \\
\textbf{LeNet} & \textbf{cifar-10} & \textbf{VGG-16} & \textbf{VGG-19} \\
conv           & conv              & conv            & conv            \\
pool           & pool              & conv            & conv            \\
conv           & conv              & pool            & pool            \\
pool           & pool              & conv            & conv            \\
fc             & conv              & conv            & conv            \\
fc             & pool              & pool            & pool            \\
               & fc                & conv            & conv            \\
               & fc                & conv            & conv            \\
               &                   & conv            & conv            \\
               &                   & pool            & conv            \\
               &                   & conv            & pool            \\
               &                   & conv            & conv            \\
               &                   & conv            & conv            \\
               &                   & pool            & conv            \\
               &                   & conv            & conv            \\
               &                   & conv            & pool            \\
               &                   & conv            & conv            \\
               &                   & pool            & conv            \\
               &                   & fc              & conv            \\
               &                   & fc              & conv            \\
               &                   & fc              & pool            \\
               &                   &                 & fc              \\
               &                   &                 & fc              \\
               &                   &                 & fc             
\end{tabular}
\end{table}

\subsubsection{The convolution layer}
The convolution layer is the cornerstone of the convolutional neural network, and is made up of several components.

The first component of a layer $l$ is the layer activation itself (the layer output). The general form of a layer activation $a_{l}$ in a convolutional neural network can be interpreted as a three-dimensional tensor $X\times Y\times D$ where $X$ and $Y$ are the activation dimensions and $D$, or the activation depth, is equal to the number of kernels applied to the input. 

The second component, the input of the current layer $a_{l}$, is equivalent to the activation of the previous layer as follows:

\begin{equation}
\label{eqn:forward}
input_{a_{l}} = a_{l-1}.
\end{equation}
\myequations{Feeding forward}

Note that the initial image or first layer of a network is an $X\times Y\times 3$ tensor, where $D$ in this case represents the RGB color channels of the original image. Beyond the input layer, the value of $D$ at $a_{l}$ is then the number of kernels just applied to the input $a_{l-1}$.

The third component is the kernel set, made up of $D$ kernels. Each kernel $kernel_d$ of the kernel set is a three-dimensional tensor and is of shape $M\times M\times Z$, where $M$, representing the kernel width and height, is of a predetermined dimension (usually small, e.g. $3$, or perhaps $7$). $Z$ is the kernel depth, and is equivalent to $D_{l-1}$, or the previous activation's depth. Each kernel $kernel_d$ makes up a three-dimensional tensor reaching through the entire depth $Z$ of the input. The role of each kernel is to slide across the input at a predetermined ``stride'', much like a moving window, gathering a dot product at each slice of itself along $Z$ with the current input lying within this window, and summing each of these products to return a single value \cite{convnets}. Each of the kernels $kernel_d$ performs its task as described above, delivering a two-dimensional matrix product result. These two-dimensional results are then stacked along $D$ to form the three-dimensional volume $a_{l}$ seen in Figure \ref{fig:convlayer}.
																																
\bildklein{figures/convlayer.png}{The convolution kernel at work}{Each set of kernels (here, of count five) focuses on a small section of the input (in pink) at a time, reaching through its entire depth. The result from each of these sets of three-dimensional kernels is a stack of two-dimensional activations, making up one three-dimensional section (small rectangle in blue) of the total output volume (large box in blue), having the depth of the number of kernels in the set. This operation is repeated all over the image. [CC Image courtesy of Aphex34 on Wikimedia Commons]}{fig:convlayer}

The remaining components make up what can be viewed as the hyperparameters of the kernel, and include its shape, its stride and the amount---if any---of padding it adds to the input layer. A typical size is $3\times3$, and a typical stride is one.

During the forward pass, each element $a_{l}[x,y,d]$ is computed by applying the $kernel_{d}$ to a region of $a_{l-1}$ of size $M\times M\times Z$, performing dot products along each dimension $Z$ and summing these products. After each such calculation, the kernel moves along from left-to-right, top-to-bottom at an increment equal to the stride parameter. This is then repeated for all kernels and the results are stacked along $D$. Equation \ref{eqn:conv} below shows this operation for one kernel, assuming that stride is set to one and there is no zero-padding. In practice, convolution reduces the size of the input volume along the length and width dimensions, and adding padding beforehand can amend this.

\begin{equation}
\label{eqn:conv}
a_{l}[x,y,d] = \sum_{i=0}^{M-1}\sum_{j=0}^{M-1}\sum_{z=0}^{Z-1}{a_{l-1}[x+i,y+j,z] * kernel_{d}[i,j,z]}
\end{equation}
\myequations{The convolution operation}

\subsubsection{Pooling and relu layers}
Pooling layers are also important in understanding how CNNs change the input volume. They are different in that they don't contain weights, and are therefore lightweight and not a natural candidate for optimization like the convolution layer.

The pooling layer takes three required hyperparameters: pooling size, pooling mode and stride (for the sake of simplicity, this description will assume a padding of zero). Pooling size, or the pooling dimensions $a$ and $b$, define the size of the pooling kernel, much like the kernel in the convolution layer. The pooling mode is typically average-pool or max-pool \cite{convnets}, denoting mathematical operations of mean-value or maximum-value, respectively.

The layer takes an input volume $p$ of size $[x,y]$ and, assuming a stride of one, returns a reduced input volume $p'$ of size $\frac{x}{a},\frac{y}{b}$, where $a$ and $b$ are the kernel dimensions. To determine an element at the output volume $p'$, with a pooling mode of maximum-value, a kernel with dimensions $a\times b$ and a stride of one, the pooling layer performs the following operation:

\begin{equation}
\label{eqn:pool}
p'[x,y] = \max_{i=0}^{a-1}\max_{j=0}^{b-1}{p[x+i,y+j]}.
\end{equation}
\myequations{Maximum-value pooling}

Notice that unlike convolution, the are no kernel parameters. The pooling layer serves to reduce the size of the input layer and in this sense, can indirectly have a dramatic effect on the speed of the network by reducing the input size of a subsequent convolution layer, thus reducing the total number of FLOPS needed for such an operation.

ReLU layers, like pooling layers, don't contain parameters. However unlike pooling layers, they do not change the input volume size. Their purpose is to apply an element-wise non-linearity to a layer, and are somewhat uninteresting in the context of this work.

\section{GEMM}
As previously mentioned, in the case of the famous AlexNet architecture, 89\% of computational processing time is taken up by convolutions. Thus, it is a worthy undertaking to optimize the mathematical operation itself, making it as efficient as possible for the computer. The agreed upon solution is the GEMM operation, or General Matrix to Matrix Multiplication, dating back to 1979 and described in ``Basic Linear Algebra Subprograms for Fortran Usage,'' or BLAS \cite{BLAS}. BLAS improves upon the normal convolution operation by transforming it into one single matrix-matrix multiplication. Normally, a convolution involves the process described in Equation \ref{eqn:conv} on page \pageref{eqn:conv}, where a filter performs tiny matrix-matrix multiplications all over the image. Instead, GEMM performs the ``image-to-column'' operation (commonly known as ``im2col'') \cite{im2col}, successfully translating the input volume and weight kernels such that the result of a convolution involves one and only one matrix multiplication between the im2col output of input and the im2col output of weights.

Behind the scenes, im2col is simply serializing each input selection (small pink box in Figure \ref{fig:convlayer} on page \pageref{fig:convlayer}) into one row of the new input matrix. Similarly, each kernel is serialized as a column of the new weight matrix. The result is now a classic matrix-matrix multiplication, visualized in Figure \ref{fig:mm}. Effectively, performing im2col and then matrix multiplication is equivalent to performing convolution as described in \ref{eqn:conv} on page \pageref{eqn:conv} for all values of $k$.

\bildklein{figures/matrixmult.png}{Matrix multiplication}{After GEMM prepares the input and weight volumes, the result of a convolution is obtained simply through traditional matrix-matrix multiplication. [CC Image courtesy of Quartl on Wikimedia Commons]}{fig:mm}

In the next section, now with a firm understanding of the inner workings of convolutional neural networks and the convolution operation itself, quantization, as a method to reduce computational overhead and increase inference speed, is discussed.

\section{Quantization}
\subsection{Computer number representation}
In order to understand why quantization is important, it would be beneficial to review how the computer stores numbers.

32-bit floating-point representation is a computer representation of real numbers. Although an estimation in itself---as real numbers are infinite and computer-representations cannot be---they offer a certain degree of precision in number representation and calculations \cite{ieee}. In a computer, they take scientific notation form (see Equation \ref{eqn:floatrep} below) and can represent $2^{32}$ individual values. They use one bit for the sign, eight bits for the exponent, and 23 bits for the fraction \cite{ieee}.

\begin{equation}
\label{eqn:floatrep}
-9.876 = \overbrace{\underbrace{-1}_\text{sign}}^\text{1 bit}\times\overbrace{\underbrace{9876}_\text{fraction}}^\text{23 bits}\times\overbrace{\underbrace{2^{-3}}_\text{exponent}}^\text{8 bits}
\end{equation}
\myequations{32-bit floating-point representation}

Integers, on the other hand, are represented by a fixed amount of bits, such as eight \cite{ieee}. 32-bit/ 64-bit processors are able to access large chunks of memory at a time, and it would in theory be faster to use integers rather than floating-point representation, as $\times4$ the amount of numbers would be accessed within a single memory-retrieval operation, reducing memory bandwidth by 75\%. However, 8-bit integers have the potential to represent only $2^{8}$ distinct numbers, and thus using them comes at a precision cost. It will be shown, despite this significantly reduced precision, that using integer representation in convolution calculations is still worthwhile.

\subsection{The quantization process}
In short, quantization is a conversion of floating-point representation, specifically 32-bit floating-point, to integer representation, or 8-bit fixed-point. The process of quantization is relatively straightforward. Taking the minimum and maximum of the floating-point representation, a new range is defined using an appropriate integer representation, such as 0 to 255. \cite{warden_quantize}. In other words, 0 will now represent the minimum value from the original unquantized matrix and 255 the maximum value (see Table \ref{tbl:quantize} below).

\begin{table}[]
\centering
\caption[Quantized value representation]{Quantized value representation.}
\label{tbl:quantize}
\begin{tabular}{ll}
\textbf{32-bit} & \textbf{8-bit} \\
-2.356          & 0              \\
1.201           & 127            \\
4.758           & 255           
\end{tabular}
\end{table}

\subsection{The quantization process broken down}
\label{sec:quantization}
Mathematically, the conversion from a floating-point tensor $f$ to integer representation $q$ involves a few processes, the first of which is to derive a quantization scale parameter from the minimum and maximum of the floating-point representation as follows:

\begin{equation}
\label{eqn:scale}
scale = \frac{\max_{f} - \min_{f}}{255 - 0}.
\end{equation}
\myequations{Quantization: the scale parameter}

Next, the zero-point of the quantized representation is determined and rounded as an integer,

\begin{equation}
\label{eqn:zero}
zero point_{q} = round(\min_{f} - \frac{\min_{q}}{scale}),
\end{equation}
\myequations{Quantization: the zero-point parameter}

and finally the floating-point values are converted to integers with the scale and zero point parameters,

\begin{equation}
\label{eqn:convert}
q[x,y,z] = zero point_{q} + \frac{f[x,y,z]}{scale},
\end{equation}
\myequations{Quantization: converting to integer}

and confined within the constraints of the defined quantized range if they happen to fall outside of it \cite{gemmlowp} like so:

\begin{equation}
\label{eqn:clamp}
\begin{aligned}
u = \min_{255, q[x,y,z]}
\\
q[x,y,z]_{confined} = \max_{0, u},
\end{aligned}
\end{equation}
\myequations{Quantization: confining the input}

where $x$, $y$ and $z$ represent tensor coordinates.

After quantization, calculations are performed as usual. The result is then converted back into 32-bit float using more minimum and maximum parameters and passed along the network.

It may be apparent to the reader at this point that there is not insignificant overhead involved in this process: namely, determining parameters and converting back and forth between quantized and dequantized representations for each convolution layer. This will be addressed later in this work.